{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdc63be851f332e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Text Preprocessing and TF-IDF with the Simpsons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5020c139309186",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:14:08.941511Z",
     "start_time": "2024-06-18T21:14:08.936332Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c0eb097b2e2cf2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:15:47.042756Z",
     "start_time": "2024-06-18T21:15:46.669853Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_dialog_data(path: str) -> dict:\n",
    "    dialogs = dict()\n",
    "    with open(path, mode='r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    data = [l.split(',', maxsplit=1) for l in lines]\n",
    "    for l in data:\n",
    "        if l[0] in dialogs:\n",
    "            dialogs[l[0]].append(l[1])\n",
    "        else:\n",
    "            dialogs[l[0]] = list(l[1])\n",
    "    return dialogs\n",
    "\n",
    "simpsons_data = parse_dialog_data('./../data/simpsons/simpsons_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8acbb989efae6e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:15:50.562891Z",
     "start_time": "2024-06-18T21:15:50.553574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['\"Aw, Dad. It\\'s just a popularity contest.\"\\n',\n '\"Sure, why not?\"\\n',\n '\"Hm, yeah.\"\\n',\n '\"He says, there aren\\'t any easy answers! I say, he\\'s not looking hard enough!\"\\n',\n '\"Me, too, Mom. I think they\\'re drifting apart.\"\\n']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peering into the dataset 👁️\n",
    "simpsons_data['Bart Simpson'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86542c21b7e65120",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:15:53.602502Z",
     "start_time": "2024-06-18T21:15:53.596418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear empty dialogs\n",
    "_ = simpsons_data.pop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b623a641d5f2bb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:15:55.078981Z",
     "start_time": "2024-06-18T21:15:55.071318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def char_level_preprocess(doc):\n",
    "    doc = re.sub(r'(\\d+)', ' ', doc)\n",
    "    doc = re.sub(r'(\\s+)', ' ', doc)\n",
    "    doc = re.sub(rf'[{re.escape(string.punctuation)}]', '', doc)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    return doc\n",
    "\n",
    "\n",
    "def word_level_preprocess(doc):\n",
    "    sw = stopwords.words('english')\n",
    "    res = ''\n",
    "    for w in doc.split():\n",
    "        if w not in sw:\n",
    "            res += w + ' '\n",
    "    return res.strip()\n",
    "\n",
    "\n",
    "def apply_preprocess(doc, min_word_len=3, remove_stop=True):\n",
    "    doc = char_level_preprocess(doc)\n",
    "    if remove_stop:\n",
    "        doc = word_level_preprocess(doc)\n",
    "    return \" \".join([word for word in doc.split() if len(word) >= min_word_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c77694ad3119ae5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:16:02.515111Z",
     "start_time": "2024-06-18T21:15:56.552374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concat each dialog into a single document and then apply preprocessing steps defined above.\n",
    "MIN_WORD_LEN = 4\n",
    "for k in simpsons_data.keys():\n",
    "    doc = \" \".join(line for line in simpsons_data[k])\n",
    "    simpsons_data[k] = apply_preprocess(doc, MIN_WORD_LEN, remove_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2a349e4ba128f8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:16:02.526543Z",
     "start_time": "2024-06-18T21:16:02.518512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'thanks vote well right thanks vote girls well dont sweat long couple people right milhouse lewis somebody must voted demand recount whoa somebody bound cant believe milhouse lewis voted yeah well thanks monkeyman yeah cant please theyre fighting understand wasted much respect ever ever wanna come back need babysitter almost half dont take tone young lady give taste back hand grampa hurry forgot give list things lisa cant supermarket well video store grab krusty burger head arcade crazy topsyturvy times whos whats right wrong right guts telling bleed gramps check check check whats next grampa aisle step yeah grampa weve grampa last time milhouse blowout casa simpson adult frail milhouse good whats happening afternoon young lisa whats wrong youre great party lisa really great yeah take care thanks coming nice nelson lisa strong unpleasant feeling never well make feeling away please never trust another person number nine thats fallout became ward care good comics ever casper wimpy ghost w'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_data['Bart Simpson'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228b7cdcc3903909",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:16:38.217586Z",
     "start_time": "2024-06-18T21:16:38.207901Z"
    }
   },
   "outputs": [],
   "source": [
    "def term_freq(data: str) -> (dict, int):\n",
    "    count_dict = dict()\n",
    "    total = 0\n",
    "    for word in data.split():\n",
    "        if word in count_dict:\n",
    "            count_dict[word] += 1\n",
    "        else:\n",
    "            count_dict[word] = 0\n",
    "        total += 1\n",
    "    return count_dict, total\n",
    "\n",
    "\n",
    "def doc_freq(docs: list, term: str):\n",
    "    # We build a set to make use of O(1) insertion and search operation\n",
    "    # and use it to check if the word exist in the list.\n",
    "    # Finally, sum counts the number of such documents\n",
    "    return sum(1 if term in set(doc.split()) else 0 for doc in docs)\n",
    "\n",
    "\n",
    "# Accepts character dictionary (k,v -> char, document) and a specific list of words\n",
    "# Calculates IDF for those specific words\n",
    "def inv_doc_freq(docs_dict: dict, word_list):\n",
    "    corpus_idf_dict = dict()\n",
    "    size = len(list(docs_dict.keys()))\n",
    "    for char in docs_dict.keys():\n",
    "        word_freq, _ = term_freq(docs_dict[char])\n",
    "        for word in word_list:\n",
    "            if word in word_freq and word not in corpus_idf_dict:\n",
    "                corpus_idf_dict[word] = size / doc_freq(list(docs_dict.values()), word)\n",
    "                corpus_idf_dict[word] = 0 if word_freq[word] == 0 else math.log(corpus_idf_dict[word])\n",
    "\n",
    "    return corpus_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa2ee9bb8d1c0862",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:16:42.191140Z",
     "start_time": "2024-06-18T21:16:40.401863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       Character   Word        TF       IDF    TF_IDF\n0   Lisa Simpson   bart  0.016146  3.363309  0.054305\n1   Lisa Simpson   dont  0.013241  2.198557  0.029111\n2   Lisa Simpson   like  0.009714  2.193238  0.021306\n3   Bart Simpson   dont  0.012988  2.198557  0.028556\n4   Bart Simpson   like  0.010123  2.193238  0.022201\n5   Bart Simpson   well  0.009875  2.131522  0.021049\n6  Homer Simpson  marge  0.014828  3.545631  0.052575\n7  Homer Simpson   dont  0.012696  2.198557  0.027912\n8  Homer Simpson   well  0.011448  2.131522  0.024401",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Character</th>\n      <th>Word</th>\n      <th>TF</th>\n      <th>IDF</th>\n      <th>TF_IDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lisa Simpson</td>\n      <td>bart</td>\n      <td>0.016146</td>\n      <td>3.363309</td>\n      <td>0.054305</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Lisa Simpson</td>\n      <td>dont</td>\n      <td>0.013241</td>\n      <td>2.198557</td>\n      <td>0.029111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lisa Simpson</td>\n      <td>like</td>\n      <td>0.009714</td>\n      <td>2.193238</td>\n      <td>0.021306</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bart Simpson</td>\n      <td>dont</td>\n      <td>0.012988</td>\n      <td>2.198557</td>\n      <td>0.028556</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bart Simpson</td>\n      <td>like</td>\n      <td>0.010123</td>\n      <td>2.193238</td>\n      <td>0.022201</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Bart Simpson</td>\n      <td>well</td>\n      <td>0.009875</td>\n      <td>2.131522</td>\n      <td>0.021049</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Homer Simpson</td>\n      <td>marge</td>\n      <td>0.014828</td>\n      <td>3.545631</td>\n      <td>0.052575</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Homer Simpson</td>\n      <td>dont</td>\n      <td>0.012696</td>\n      <td>2.198557</td>\n      <td>0.027912</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Homer Simpson</td>\n      <td>well</td>\n      <td>0.011448</td>\n      <td>2.131522</td>\n      <td>0.024401</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_tf_idf(chars: list, char_map: dict, max_word_lim=3):\n",
    "    df_list = list()\n",
    "    for ch in chars:\n",
    "        freq_dict, total_count = term_freq(char_map[ch])\n",
    "        sorted_words = sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "        freq_idf_dict = inv_doc_freq(char_map, sorted_words[:3])\n",
    "        \n",
    "        for word in sorted_words[:max_word_lim]:\n",
    "            tf = freq_dict[word] / total_count\n",
    "            df_list.append({'Character': ch,\n",
    "                            'Word': word,\n",
    "                            'TF': tf,\n",
    "                            'IDF': freq_idf_dict[word],\n",
    "                            'TF_IDF': tf * freq_idf_dict[word]})\n",
    "\n",
    "    return pd.DataFrame(df_list)\n",
    "\n",
    "\n",
    "cal_tf_idf(['Lisa Simpson', 'Bart Simpson', 'Homer Simpson'], simpsons_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
